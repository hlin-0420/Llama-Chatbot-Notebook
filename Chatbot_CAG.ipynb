{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hlin-0420/Llama-Chatbot-Notebook/blob/main/Llama_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "import logging\n",
    "from bs4 import XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMFileFinder:\n",
    "    def __init__(self, base_directory):\n",
    "        self.base_directory = base_directory\n",
    "\n",
    "    def _list_htm_files(self):\n",
    "        \"\"\"\n",
    "        Recursively finds all .htm files in the base directory and its subdirectories.\n",
    "        \"\"\"\n",
    "        htm_files = []\n",
    "        for root, _, files in os.walk(self.base_directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".htm\"):\n",
    "                    relative_path = os.path.relpath(os.path.join(root, file), start=self.base_directory)\n",
    "                    htm_files.append(os.path.join(self.base_directory, relative_path))\n",
    "        return htm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm_file_finder_model = HTMFileFinder(\"Data\")\n",
    "\n",
    "htm_file_directories = htm_file_finder_model._list_htm_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Different Document Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(soup):\n",
    "    \"\"\"\n",
    "    Extracts clean and meaningful text from the parsed HTML soup, excluding navigation and short texts.\n",
    "    \"\"\"\n",
    "    # Define navigation-related keyword patterns\n",
    "    navigation_keywords = [\n",
    "        r'contact\\s+us', r'click\\s+(here|for)', r'guidance', r'help', r'support', r'assistance',\n",
    "        r'maximize\\s+screen', r'view\\s+details', r'read\\s+more', r'convert.*file', r'FAQ', r'learn\\s+more'\n",
    "    ]\n",
    "    navigation_pattern = re.compile(r\"|\".join(navigation_keywords), re.IGNORECASE)\n",
    "\n",
    "    # Remove navigation-related text\n",
    "    for tag in soup.find_all(\"p\"):\n",
    "        if navigation_pattern.search(tag.text):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Extract meaningful paragraphs (length > 20)\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 20]\n",
    "    return \"\\n\\n\".join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_as_text_block(soup, file_path):\n",
    "    \"\"\"\n",
    "    Extract tables from HTML as a formatted text block, skipping navigation and NaN-only tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tables = pd.read_html(file_path)\n",
    "\n",
    "        def is_navigation_table(table):\n",
    "            \"\"\"Check if the table contains only navigation-related keywords.\"\"\"\n",
    "            flattened = [str(cell).strip().lower() for cell in table.to_numpy().flatten()]\n",
    "            return set(flattened).issubset({\"back\", \"forward\"})\n",
    "\n",
    "        def is_nan_only_table(table):\n",
    "            \"\"\"Check if the entire table contains only NaN values.\"\"\"\n",
    "            return table.isna().all().all()\n",
    "\n",
    "        table_texts = []\n",
    "        for idx, table in enumerate(tables):\n",
    "            if is_navigation_table(table) or is_nan_only_table(table):\n",
    "                continue\n",
    "\n",
    "            # Drop rows where both the second and third columns are NaN\n",
    "            if table.shape[1] == 2:\n",
    "                table = table.dropna(how='all')\n",
    "                table[table.columns[-1]] = table[table.columns[-1]].fillna(\"\")\n",
    "\n",
    "            formatted_table = tabulate(table, headers=\"keys\", tablefmt=\"grid\")\n",
    "            beautified_table = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘            ğŸ“Š Table {idx+1} from {file_path}              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "{formatted_table}\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘            ğŸ”š End of Table {idx+1}                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "            table_texts.append(beautified_table)\n",
    "\n",
    "        return \"\\n\".join(table_texts) if table_texts else \"\"\n",
    "    except ValueError:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(soup):\n",
    "    \"\"\"\n",
    "    Extracts lists from HTML and formats them as bullet points.\n",
    "    \"\"\"\n",
    "    lists = []\n",
    "    for ul in soup.find_all(\"ul\"):\n",
    "        items = [li.get_text(strip=True) for li in ul.find_all(\"li\")]\n",
    "        if items:\n",
    "            formatted_list = \"\\n\".join([f\"â€¢ {item}\" for item in items])\n",
    "            lists.append(formatted_list)\n",
    "    return \"\\n\\n\".join(lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_content(htm_files, selectedOptions=None):\n",
    "    \"\"\"\n",
    "    Load and process all .htm files from the base directory, extracting text, tables, and lists.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default options if none are specified\n",
    "    if selectedOptions is None:\n",
    "        selectedOptions = [\"text\", \"table\", \"list\"]\n",
    "\n",
    "    web_documents = []\n",
    "\n",
    "    for file_path in htm_files:\n",
    "        try:\n",
    "            with open(file_path, encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                content = content[content.find(\"<body>\")+6:content.find(\"</body>\")]\n",
    "                soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "                # Extract content based on selected options\n",
    "                clean_text = extract_text(soup) if \"text\" in selectedOptions else \"\"\n",
    "                formatted_table = extract_table_as_text_block(soup, file_path) if \"table\" in selectedOptions else \"\"\n",
    "                formatted_list = extract_list(soup) if \"list\" in selectedOptions else \"\"\n",
    "\n",
    "                # Combine extracted content into a single text block\n",
    "                page_text = \"\\n\\n\".join(filter(None, [clean_text, formatted_table, formatted_list]))\n",
    "\n",
    "                if page_text:\n",
    "                    document = LangchainDocument(page_content=page_text)\n",
    "                    web_documents.append(document)\n",
    "                    logging.info(f\"âœ… Loaded document from {file_path}\")\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            logging.error(f\"âŒ Could not read the file {file_path}. Check the file encoding.\")\n",
    "\n",
    "    logging.info(f\"âœ… Total documents loaded: {len(web_documents)}\")\n",
    "    \n",
    "    return web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaochengLin\\AppData\\Local\\Temp\\ipykernel_17480\\2321737619.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[table.columns[-1]] = table[table.columns[-1]].fillna(\"\")\n"
     ]
    }
   ],
   "source": [
    "training_web_documents = _load_content(htm_file_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context_response_pairs(web_documents):\n",
    "    \"\"\"\n",
    "    Prepare context-response pairs from loaded web documents.\n",
    "    \"\"\"\n",
    "    context_response_pairs = []\n",
    "    for doc in web_documents:\n",
    "        context = doc.page_content  # Treat the entire document as context\n",
    "        # Placeholder for response (you might replace it with manually labeled responses)\n",
    "        response = \"Generated response based on context\"\n",
    "        context_response_pairs.append((context, response))\n",
    "    return context_response_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total context-response pairs: 264\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context_response_pairs = prepare_context_response_pairs(training_web_documents)\n",
    "print(f\"Total context-response pairs: {len(context_response_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07d85064c9c440d988df3b26fd9432a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104466c9cd2247fe9a36f69805a65b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2f4a95b1ad4c8aba8f192e4f43e59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584cc065f4e0412db2f55278749ac3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6641c3a61a41465ebb3707a8159320e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and tokenizer from Hugging Face\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the model and tokenizer\n",
    "save_directory = \"./local_models/t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer saved locally at ./local_models/t5-base\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer and model locally\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"âœ… Model and tokenizer saved locally at {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded successfully from local directory.\n"
     ]
    }
   ],
   "source": [
    "# Load the locally saved model and tokenizer\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "local_model = AutoModelForSeq2SeqLM.from_pretrained(save_directory)\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded successfully from local directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_context(context):\n",
    "    \"\"\"\n",
    "    Encode the context using a transformer model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 0.8295803365475546\n",
      "Epoch 2/3 - Loss: 0.05924200758553608\n",
      "Epoch 3/3 - Loss: 0.004273090744887172\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "def train_cag_model(context_response_pairs, epochs=3, learning_rate=5e-5):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, response in context_response_pairs:\n",
    "            # Encode context and response\n",
    "            context_inputs = encode_context(context)\n",
    "            response_inputs = encode_context(response)\n",
    "\n",
    "            # Generate outputs\n",
    "            outputs = model(input_ids=context_inputs, labels=response_inputs)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(context_response_pairs)}\")\n",
    "\n",
    "# Train the model\n",
    "train_cag_model(context_response_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "What is the importance of context-aware generation in AI?\n"
     ]
    }
   ],
   "source": [
    "def generate_local_response(context):\n",
    "    \"\"\"\n",
    "    Generate a response based on the given context using the locally saved model.\n",
    "    \"\"\"\n",
    "    local_model.eval()\n",
    "    inputs = local_tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    output = local_model.generate(inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "    response = local_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the locally loaded model\n",
    "context = \"What is the importance of context-aware generation in AI?\"\n",
    "print(\"Generated Response:\")\n",
    "print(generate_local_response(context))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM72gT33YROTvw/iOzBozoS",
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "1VdmZgAAqHSHdNcYYJEosB3DZAlbdLq19",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "211fc957897a4b648c7472ffcf0dfb5c": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_bf6949f69ab3400b84d1a9bd92ac5d79",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "AI Bot Response:\n",
         " You can define up to 200 tracks in one ODF file.\n"
        ]
       }
      ]
     }
    },
    "5f0df221121e47b4a6cd814d71340b8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "primary",
      "description": "Submit",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_cba980fdfb1442a8984e268808b01ff0",
      "style": "IPY_MODEL_5f20b1e6f1e14e9db57a5953092095c9",
      "tooltip": "Click to query AI bot"
     }
    },
    "5f20b1e6f1e14e9db57a5953092095c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "72fd9f9d6f4941c4bb4793aaffcf5380": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Question:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_c5f4a113e5694f719b3de2937b53e7d3",
      "placeholder": "Enter your question here...",
      "style": "IPY_MODEL_7c57b26ff215460fb377bd9be3e0b71d",
      "value": "How many tracks can you define in one ODF?"
     }
    },
    "7c57b26ff215460fb377bd9be3e0b71d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf6949f69ab3400b84d1a9bd92ac5d79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f4a113e5694f719b3de2937b53e7d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "cba980fdfb1442a8984e268808b01ff0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
