{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlin-0420/Llama-Chatbot-Notebook/blob/main/Llama_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AojMTwCHL3-W"
      },
      "source": [
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You're running locally. Please make sure the model is installed via `ollama pull llama3.2:latest` in your terminal.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install colab-xterm\n",
        "    %load_ext colabxterm\n",
        "    %xterm\n",
        "    !ollama pull llama3.2:latest  # Only pull in Colab\n",
        "else:\n",
        "    print(\"You're running locally. Please make sure the model is installed via `ollama pull llama3.2:latest` in your terminal.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-7LwVxbL7UD"
      },
      "source": [
        "## Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi4tTI7v-jmB",
        "outputId": "f20b973a-16cb-4be2-a038-cdd13ed10040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: langchain in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-huggingface in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.1.2)\n",
            "Requirement already satisfied: langchain-ollama in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.3.49)\n",
            "Requirement already satisfied: tabulate in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: regex in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (2024.11.6)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (4.0.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: rapidfuzz in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (3.12.2)\n",
            "Requirement already satisfied: nltk in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.1.2)\n",
            "Requirement already satisfied: requests in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: torch in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (2.6.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: lxml in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (5.3.1)\n",
            "Requirement already satisfied: html5lib in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from beautifulsoup4) (4.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain) (0.3.20)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain) (2.11.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-huggingface) (0.30.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-huggingface) (4.50.3)\n",
            "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: click in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: absl-py in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from rouge-score) (2.2.1)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from html5lib) (0.5.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4 pandas langchain langchain-community langchain-huggingface langchain-ollama langchain-core tabulate regex sentence-transformers numpy rapidfuzz nltk rouge-score requests scikit-learn torch tiktoken lxml html5lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "395CXRdg-9AA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\haoch\\Desktop\\Llama-Chatbot-Notebook\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import json\n",
        "from tabulate import tabulate\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fsc3QoMGJmkG"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from bs4 import XMLParsedAsHTMLWarning\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zBA7uaxMAQc"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # If running in Google Colab\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = '/content/drive/My Drive/GEO-chatbot'  \n",
        "else:\n",
        "    base_path = os.path.abspath('.')  # Local directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fubFOjolED2X"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = os.path.join(base_path, \"Data\")\n",
        "FEEDBACK_FILE = os.path.join(DATA_DIR, \"feedback_dataset.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWp43p2UMCQ4"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7mJLfF3wCnpu"
      },
      "outputs": [],
      "source": [
        "class RAGApplication:\n",
        "    def __init__(self, retriever, rag_chain, web_documents):\n",
        "        self.retriever = retriever\n",
        "        self.rag_chain = rag_chain\n",
        "        self.web_documents = web_documents  # Store the documents for feedback retrieval\n",
        "        self.feedback_model = SentenceTransformer(\"./embeddings/offline_model\")  # Embedding model for similarity\n",
        "        self.feedback_data, self.feedback_embeddings = self._load_feedback()\n",
        "\n",
        "    def _load_feedback(self):\n",
        "        \"\"\"Loads feedback from file and precomputes embeddings to optimize retrieval.\"\"\"\n",
        "        if not os.path.exists(FEEDBACK_FILE):\n",
        "            logging.warning(\"âš ï¸ No feedback file found.\")\n",
        "            return [], []\n",
        "\n",
        "        try:\n",
        "            with open(FEEDBACK_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                feedback_data = json.load(file)  # Load feedback JSON array\n",
        "        except json.JSONDecodeError:\n",
        "            logging.error(\"âš ï¸ Error decoding feedback JSON file. Returning empty feedback.\")\n",
        "            return [], []\n",
        "\n",
        "        extracted_feedback = [\n",
        "            {\n",
        "                \"question\": entry[\"question\"],\n",
        "                \"feedback\": entry[\"feedback\"],\n",
        "                \"rating\": int(entry.get(\"rating-score\", 0))\n",
        "            }\n",
        "            for entry in feedback_data if \"question\" in entry and \"feedback\" in entry\n",
        "        ]\n",
        "\n",
        "        if not extracted_feedback:\n",
        "            logging.warning(\"âš ï¸ No valid feedback extracted.\")\n",
        "            return [], []\n",
        "\n",
        "        # Compute embeddings in parallel\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            feedback_embeddings = list(executor.map(\n",
        "                lambda fb: self.feedback_model.encode(fb[\"question\"], convert_to_tensor=True),\n",
        "                extracted_feedback\n",
        "            ))\n",
        "\n",
        "        return extracted_feedback, feedback_embeddings\n",
        "\n",
        "    def _get_relevant_feedback(self, question, top_k=3):\n",
        "        \"\"\"Retrieve the most relevant feedback based on semantic similarity.\"\"\"\n",
        "        if not self.feedback_data:\n",
        "            return \"\"\n",
        "\n",
        "        # Compute embedding for the new question\n",
        "        question_embedding = self.feedback_model.encode(question, convert_to_tensor=True)\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = np.array([\n",
        "            util.pytorch_cos_sim(question_embedding, fb_emb)[0].item()\n",
        "            for fb_emb in self.feedback_embeddings\n",
        "        ])\n",
        "\n",
        "        # Get indices of top-k similar feedback\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        # Extract unique questions while maintaining order\n",
        "        selected_feedback = []\n",
        "        unique_questions = set()\n",
        "\n",
        "        for idx in top_indices:\n",
        "            fb = self.feedback_data[idx]\n",
        "            base_question = fb[\"question\"].lower().strip(\"?\")\n",
        "            if base_question not in unique_questions:\n",
        "                selected_feedback.append(fb[\"feedback\"])\n",
        "                unique_questions.add(base_question)\n",
        "            if len(selected_feedback) >= top_k:\n",
        "                break\n",
        "\n",
        "        return \"\\n\".join(selected_feedback) if selected_feedback else \"\"\n",
        "\n",
        "    def run(self, question):\n",
        "        \"\"\"Runs the RAG retrieval and generates a response with detailed runtime analysis.\"\"\"\n",
        "\n",
        "        total_start_time = time.perf_counter()  # Start total execution timer\n",
        "\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        retrieval_start_time = time.perf_counter()\n",
        "        documents = self.retriever.invoke(question)\n",
        "        retrieval_end_time = time.perf_counter()\n",
        "        retrieval_time = retrieval_end_time - retrieval_start_time\n",
        "\n",
        "        doc_texts = \"\\n\".join(doc.page_content for doc in documents)\n",
        "\n",
        "        # Step 2: Retrieve relevant feedback\n",
        "        feedback_start_time = time.perf_counter()\n",
        "        feedback_texts = self._get_relevant_feedback(question)\n",
        "        feedback_end_time = time.perf_counter()\n",
        "        feedback_time = feedback_end_time - feedback_start_time\n",
        "\n",
        "        if not feedback_texts.strip():\n",
        "            logging.warning(\"âš ï¸ No feedback found for this query.\")\n",
        "\n",
        "        # Step 3: Generate the answer using the updated prompt format\n",
        "        response_start_time = time.perf_counter()\n",
        "        response = self.rag_chain.invoke({\n",
        "            \"question\": question,\n",
        "            \"documents\": doc_texts,\n",
        "            \"feedback\": feedback_texts,\n",
        "            \"stream\": True\n",
        "        })\n",
        "        response_end_time = time.perf_counter()\n",
        "        response_time = response_end_time - response_start_time\n",
        "\n",
        "        total_end_time = time.perf_counter()\n",
        "        total_execution_time = total_end_time - total_start_time\n",
        "\n",
        "        # Logging detailed runtime analysis\n",
        "        logging.info(f\"ğŸ•’ RAG Execution Time Breakdown:\")\n",
        "        logging.info(f\"   - Document Retrieval Time: {retrieval_time:.4f} seconds\")\n",
        "        logging.info(f\"   - Feedback Extraction Time: {feedback_time:.4f} seconds\")\n",
        "        logging.info(f\"   - Response Generation Time: {response_time:.4f} seconds\")\n",
        "        logging.info(f\"   - Total Execution Time: {total_execution_time:.4f} seconds\")\n",
        "\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "peXWuEVyE1ci"
      },
      "outputs": [],
      "source": [
        "def extract_text(soup):\n",
        "    # Define navigation-related keyword patterns\n",
        "    navigation_keywords = [\n",
        "        r'contact\\s+us', r'click\\s+(here|for)', r'guidance', r'help', r'support', r'assistance',\n",
        "        r'maximize\\s+screen', r'view\\s+details', r'read\\s+more', r'convert.*file', r'FAQ', r'learn\\s+more'\n",
        "    ]\n",
        "\n",
        "    navigation_pattern = re.compile(r\"|\".join(navigation_keywords), re.IGNORECASE)\n",
        "\n",
        "    # Remove navigation-related text\n",
        "    for tag in soup.find_all(\"p\"):\n",
        "        if navigation_pattern.search(tag.text):\n",
        "            tag.decompose()\n",
        "\n",
        "    # Extract only meaningful paragraph text (excluding very short ones)\n",
        "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 20]\n",
        "\n",
        "    clean_text = \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "URaqBN2VE7XN"
      },
      "outputs": [],
      "source": [
        "def extract_table_as_text_block(soup, file_path):\n",
        "    \"\"\"\n",
        "    Extract tables from HTML as a single formatted text block for inclusion into page_text.\n",
        "    Skips navigation tables and handles no-table cases.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup): Parsed HTML.\n",
        "        file_path (str): Path to the file (for metadata).\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted block of all tables from this file, or a message if no tables are found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tables = pd.read_html(file_path)\n",
        "\n",
        "        def is_navigation_table(table):\n",
        "            \"\"\"Detect if table is a 'navigation-only' table with just 'back' and 'forward'.\"\"\"\n",
        "            flattened = [str(cell).strip().lower() for cell in table.to_numpy().flatten()]\n",
        "            navigation_keywords = {\"back\", \"forward\"}\n",
        "            return set(flattened).issubset(navigation_keywords)\n",
        "\n",
        "        def is_nan_only_table(table):\n",
        "            \"\"\"Detect if the entire table only contains NaN values.\"\"\"\n",
        "            return table.isna().all().all()\n",
        "\n",
        "        table_texts = []\n",
        "        table_count = 0\n",
        "\n",
        "        for idx, table in enumerate(tables):\n",
        "            if is_navigation_table(table) or is_nan_only_table(table):\n",
        "                continue\n",
        "\n",
        "            if table.shape[1] == 2:\n",
        "                # Drop rows where both the second and third columns are NaN\n",
        "                table = table.dropna(how='all')\n",
        "\n",
        "                last_col = table.columns[-1]\n",
        "\n",
        "                table[last_col] = table[last_col].fillna(\"\")\n",
        "\n",
        "            table_count += 1\n",
        "            formatted_table = tabulate(table, headers=\"keys\", tablefmt=\"grid\")\n",
        "\n",
        "            beautified_table = f\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘            ğŸ“Š Table {table_count} from {file_path}              â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "{formatted_table}\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘            ğŸ”š End of Table {table_count}                       â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\"\n",
        "            table_texts.append(beautified_table)\n",
        "\n",
        "        if not table_texts:\n",
        "            return \"\"\n",
        "\n",
        "        return \"\\n\".join(table_texts)\n",
        "\n",
        "    except ValueError:\n",
        "        # No tables found case\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ib_UCNYLE-5_"
      },
      "outputs": [],
      "source": [
        "def extract_list(soup):\n",
        "    # Extract lists properly\n",
        "    lists = []\n",
        "    for ul in soup.find_all(\"ul\"):\n",
        "        items = [li.get_text(strip=True) for li in ul.find_all(\"li\")]\n",
        "        lists.append(items)\n",
        "    return lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# top_p_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "top_p_list = [0.1, 0.5, 1.0]\n",
        "top_k_list = [0, 50, 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_p = 0.1\n",
        "top_k = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0zqgxtODDxVs"
      },
      "outputs": [],
      "source": [
        "class OllamaBot:\n",
        "    def __init__(self, top_p, top_k):\n",
        "        \"\"\"\n",
        "        Initialize the OllamaBot with the specified model.\n",
        "        \"\"\"\n",
        "        global valid_model_names\n",
        "\n",
        "        # Storage Processing\n",
        "        self.base_directory = DATA_DIR\n",
        "        self.web_documents = []  # Stores the web documents for retrieval\n",
        "        self._load_content()\n",
        "\n",
        "        # Initialize Llama model\n",
        "        # self.llm_model = ChatOllama(\n",
        "        #   model=\"gemma3:4b\",\n",
        "        #   temperature = 0.2,\n",
        "        #   num_predict=150,\n",
        "        #   top_p = 0.8,\n",
        "        #   top_k = 100\n",
        "        # )\n",
        "        \n",
        "        # default gemma 3:1b model.\n",
        "        self.llm_model = ChatOllama(\n",
        "          model=\"gemma3:1b\",\n",
        "          temperature = 0,\n",
        "          num_predict=150,\n",
        "          top_p = top_p,\n",
        "          top_k = top_k\n",
        "        )\n",
        "        \n",
        "        self._initialize_rag_application()\n",
        "\n",
        "    def _initialize_rag_application(self):\n",
        "        \"\"\"\n",
        "        Initializes the RAGApplication.\n",
        "        \"\"\"\n",
        "        global rag_application\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            chunk_size=250, chunk_overlap=0\n",
        "        )\n",
        "\n",
        "        doc_splits = text_splitter.split_documents(self.web_documents)\n",
        "\n",
        "        embedding_model = HuggingFaceEmbeddings(model_name=\"./embeddings/offline_model\")\n",
        "\n",
        "        vectorstore = SKLearnVectorStore.from_documents(\n",
        "            documents=doc_splits,\n",
        "            embedding=embedding_model,\n",
        "        )\n",
        "\n",
        "        retriever = vectorstore.as_retriever(k=4)\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            template=\"\"\"\n",
        "            You are an AI assistant for the GEO application, a tool used for well log authoring, analysis, and reporting by geologists, geoscientists, and engineers.\n",
        "\n",
        "            ğŸ“˜ Instructions:\n",
        "            Use only the GEO help guide and related documentation to answer.\n",
        "            Do not speculate or include unnecessary explanations.\n",
        "            \n",
        "            ---\n",
        "            **Documents:**\n",
        "            {documents}\n",
        "            ---\n",
        "\n",
        "            **Feedback:**\n",
        "            {feedback}\n",
        "            ---\n",
        "\n",
        "            **Question:** {question}\n",
        "\n",
        "            **Your Optimized Answer:**\n",
        "            \"\"\",\n",
        "            input_variables=[\"question\", \"documents\", \"feedback\"]\n",
        "        )\n",
        "\n",
        "        rag_chain = prompt | self.llm_model | StrOutputParser()\n",
        "        rag_application = RAGApplication(retriever, rag_chain, self.web_documents)\n",
        "\n",
        "    def _list_htm_files(self):\n",
        "        \"\"\"\n",
        "        Recursively finds all .htm files in the base directory and its subdirectories.\n",
        "        \"\"\"\n",
        "        htm_files = []\n",
        "        for root, _, files in os.walk(self.base_directory):\n",
        "            for file in files:\n",
        "                if file.endswith(\".htm\"):\n",
        "                    relative_path = os.path.relpath(os.path.join(root, file), start=self.base_directory)\n",
        "                    htm_files.append(self.base_directory + \"/\" + relative_path)\n",
        "        return htm_files\n",
        "\n",
        "    def _load_content(self, selectedOptions=None):\n",
        "        \"\"\"\n",
        "        Load and process all .htm files from the base directory.\n",
        "        \"\"\"\n",
        "        htm_files = self._list_htm_files()\n",
        "\n",
        "        if selectedOptions is None:\n",
        "            selectedOptions = [\"text\", \"table\", \"list\"]\n",
        "\n",
        "        self.web_documents = []\n",
        "        page_texts = []\n",
        "\n",
        "        for file_path in htm_files:\n",
        "            try:\n",
        "                with open(file_path, encoding=\"utf-8\") as file:\n",
        "                    content = file.read()\n",
        "                    content = content[content.find(\"<body>\")+6:content.find(\"</body>\")]\n",
        "\n",
        "                    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "                    if \"text\" in selectedOptions:\n",
        "                        clean_text = extract_text(soup)\n",
        "                    else:\n",
        "                        clean_text = \"\"\n",
        "\n",
        "                    if \"table\" in selectedOptions:\n",
        "                        formatted_table = extract_table_as_text_block(soup, file_path)\n",
        "                    else:\n",
        "                        formatted_table = \"\"\n",
        "\n",
        "                    if \"list\" in selectedOptions:\n",
        "                        lists = extract_list(soup)\n",
        "                    else:\n",
        "                        lists = \"\"\n",
        "\n",
        "                    page_text = f\"{clean_text}\\n{formatted_table}\\n{lists}\".strip()\n",
        "                    page_texts.append(page_text)\n",
        "\n",
        "                    document = LangchainDocument(page_content=page_text)\n",
        "                    self.web_documents.append(document)\n",
        "\n",
        "            except UnicodeDecodeError:\n",
        "                logging.error(f\"Could not read the file {file_path}. Check the file encoding.\")\n",
        "\n",
        "        logging.info(f\"Processed content saved.\")\n",
        "\n",
        "    def add(self, content):\n",
        "        \"\"\"\n",
        "        Add new content to the bot's memory.\n",
        "        \"\"\"\n",
        "        new_document = LangchainDocument(page_content=content)\n",
        "        self.web_documents.append(new_document)\n",
        "        self._initialize_rag_application()\n",
        "\n",
        "    def query(self, question):\n",
        "        \"\"\"\n",
        "        Query the bot and get a response.\n",
        "        \"\"\"\n",
        "        global rag_application\n",
        "\n",
        "        if rag_application is None:\n",
        "            logging.error(\"RAG application is not initialized.\")\n",
        "            return \"Error: RAG application is not initialized.\"\n",
        "\n",
        "        response = rag_application.run(question)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSF__6zvH_xr",
        "outputId": "28abe963-9a33-4f21-bfcd-153744a9a7fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\haoch\\AppData\\Local\\Temp\\ipykernel_20992\\455247735.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  table[last_col] = table[last_col].fillna(\"\")\n"
          ]
        }
      ],
      "source": [
        "ai_bot = OllamaBot(top_p, top_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRnohkSYMGLP"
      },
      "source": [
        "## User Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "     -------------------------------------- 139.8/139.8 kB 4.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipywidgets) (9.0.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.12\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "     ---------------------------------------- 2.3/2.3 MB 9.3 MB/s eta 0:00:00\n",
            "Collecting jupyterlab-widgets~=3.0.12\n",
            "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
            "     ------------------------------------- 214.4/214.4 kB 13.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: decorator in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: stack_data in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\haoch\\desktop\\llama-chatbot-notebook\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xDFOrsepKeSg"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jt8ohefkLKKZ"
      },
      "outputs": [],
      "source": [
        "# Create a text input widget\n",
        "question_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter your question here...',\n",
        "    description='Question:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Create a button to submit the question\n",
        "submit_button = widgets.Button(\n",
        "    description=\"Submit\",\n",
        "    button_style='primary',\n",
        "    tooltip='Click to query AI bot'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rDhHjjDwLPsc"
      },
      "outputs": [],
      "source": [
        "# Create an output widget to display the response\n",
        "output = widgets.Output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xNasEqYhLSLL"
      },
      "outputs": [],
      "source": [
        "# Define the function to handle the query\n",
        "def on_submit(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        question = question_input.value\n",
        "        if question.strip():\n",
        "            response = ai_bot.query(question)  # Assuming ai_bot.query() is defined\n",
        "            print(\"AI Bot Response:\\n\", response)\n",
        "        else:\n",
        "            print(\"Please enter a valid question.\")\n",
        "\n",
        "# Bind the function to the button click event\n",
        "submit_button.on_click(on_submit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "72fd9f9d6f4941c4bb4793aaffcf5380",
            "c5f4a113e5694f719b3de2937b53e7d3",
            "7c57b26ff215460fb377bd9be3e0b71d",
            "5f0df221121e47b4a6cd814d71340b8b",
            "cba980fdfb1442a8984e268808b01ff0",
            "5f20b1e6f1e14e9db57a5953092095c9",
            "211fc957897a4b648c7472ffcf0dfb5c",
            "bf6949f69ab3400b84d1a9bd92ac5d79"
          ]
        },
        "id": "85Bwg-lALUTE",
        "outputId": "c7149472-3739-4d3c-fa52-347efc965bad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba12a3c6c8a94237af5cdd3ded35aea1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', description='Question:', layout=Layout(width='100%'), placeholder='Enter your question here...'â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "882a6570c9fb4eca951e7bf67ebba1df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(button_style='primary', description='Submit', style=ButtonStyle(), tooltip='Click to query AI bot')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "936c64fa24ce44e088e2266b4b0dcb21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display the widgets\n",
        "display(question_input, submit_button, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Timings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model name\n",
        "model_name = \"gemma3:1b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Why can't I add 251 curve shades to my log?\",\n",
        "    \"What is the maximum number of data points allowed per curve?\",\n",
        "    \"I want to use the name \\\"Hydrocarbon bearing zone highlighted\\\" as my curve shade name. Why is it not allowed?\",\n",
        "    \"What is the maximum number of curves I can load in a data file?\",\n",
        "    \"I have already added 20,000 modifiers to my log. Why can't I add more?\",\n",
        "    \"How many log headers can I add to my log?\",\n",
        "    \"How many tadpole definitions am I allowed to create?\",\n",
        "    \"Why can't I add another layout to my log?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run each query\n",
        "for question in questions:\n",
        "    start_time = time.time()\n",
        "    response = ai_bot.query(question)\n",
        "    end_time = time.time()\n",
        "    duration = round(end_time - start_time, 2)\n",
        "\n",
        "    results.append({\n",
        "        \"question\": question,\n",
        "        \"response\": response,\n",
        "        \"running time (s)\": duration,\n",
        "        \"model name\": model_name\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>response</th>\n",
              "      <th>running time (s)</th>\n",
              "      <th>model name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why can't I add 251 curve shades to my log?</td>\n",
              "      <td>The document â€œNumber of curve shades per plotâ€...</td>\n",
              "      <td>4.48</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the maximum number of data points allo...</td>\n",
              "      <td>450\\n</td>\n",
              "      <td>3.39</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I want to use the name \"Hydrocarbon bearing zo...</td>\n",
              "      <td>The operation is not allowed because the name ...</td>\n",
              "      <td>9.03</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the maximum number of curves I can loa...</td>\n",
              "      <td>450</td>\n",
              "      <td>3.67</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have already added 20,000 modifiers to my lo...</td>\n",
              "      <td>The issue is that the system is limiting the n...</td>\n",
              "      <td>5.71</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How many log headers can I add to my log?</td>\n",
              "      <td>You can add up to 100 log headers to your log.</td>\n",
              "      <td>4.61</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How many tadpole definitions am I allowed to c...</td>\n",
              "      <td>23</td>\n",
              "      <td>7.10</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Why can't I add another layout to my log?</td>\n",
              "      <td>Okay, let's address why you can't add another ...</td>\n",
              "      <td>9.81</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0        Why can't I add 251 curve shades to my log?   \n",
              "1  What is the maximum number of data points allo...   \n",
              "2  I want to use the name \"Hydrocarbon bearing zo...   \n",
              "3  What is the maximum number of curves I can loa...   \n",
              "4  I have already added 20,000 modifiers to my lo...   \n",
              "5          How many log headers can I add to my log?   \n",
              "6  How many tadpole definitions am I allowed to c...   \n",
              "7          Why can't I add another layout to my log?   \n",
              "\n",
              "                                            response  running time (s)  \\\n",
              "0  The document â€œNumber of curve shades per plotâ€...              4.48   \n",
              "1                                              450\\n              3.39   \n",
              "2  The operation is not allowed because the name ...              9.03   \n",
              "3                                                450              3.67   \n",
              "4  The issue is that the system is limiting the n...              5.71   \n",
              "5     You can add up to 100 log headers to your log.              4.61   \n",
              "6                                                 23              7.10   \n",
              "7  Okay, let's address why you can't add another ...              9.81   \n",
              "\n",
              "  model name  \n",
              "0  gemma3:1b  \n",
              "1  gemma3:1b  \n",
              "2  gemma3:1b  \n",
              "3  gemma3:1b  \n",
              "4  gemma3:1b  \n",
              "5  gemma3:1b  \n",
              "6  gemma3:1b  \n",
              "7  gemma3:1b  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display as table\n",
        "df = pd.DataFrame(results)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "     -------------------------------------- 165.1/165.1 kB 2.0 MB/s eta 0:00:00\n",
            "Installing collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.2.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install XlsxWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>response</th>\n",
              "      <th>running time (s)</th>\n",
              "      <th>model name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why can't I add 251 curve shades to my log?</td>\n",
              "      <td>The document â€œNumber of curve shades per plotâ€...</td>\n",
              "      <td>4.48</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the maximum number of data points allo...</td>\n",
              "      <td>450\\n</td>\n",
              "      <td>3.39</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I want to use the name \"Hydrocarbon bearing zo...</td>\n",
              "      <td>The operation is not allowed because the name ...</td>\n",
              "      <td>9.03</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the maximum number of curves I can loa...</td>\n",
              "      <td>450</td>\n",
              "      <td>3.67</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have already added 20,000 modifiers to my lo...</td>\n",
              "      <td>The issue is that the system is limiting the n...</td>\n",
              "      <td>5.71</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How many log headers can I add to my log?</td>\n",
              "      <td>You can add up to 100 log headers to your log.</td>\n",
              "      <td>4.61</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How many tadpole definitions am I allowed to c...</td>\n",
              "      <td>23</td>\n",
              "      <td>7.10</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Why can't I add another layout to my log?</td>\n",
              "      <td>Okay, let's address why you can't add another ...</td>\n",
              "      <td>9.81</td>\n",
              "      <td>gemma3:1b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0        Why can't I add 251 curve shades to my log?   \n",
              "1  What is the maximum number of data points allo...   \n",
              "2  I want to use the name \"Hydrocarbon bearing zo...   \n",
              "3  What is the maximum number of curves I can loa...   \n",
              "4  I have already added 20,000 modifiers to my lo...   \n",
              "5          How many log headers can I add to my log?   \n",
              "6  How many tadpole definitions am I allowed to c...   \n",
              "7          Why can't I add another layout to my log?   \n",
              "\n",
              "                                            response  running time (s)  \\\n",
              "0  The document â€œNumber of curve shades per plotâ€...              4.48   \n",
              "1                                              450\\n              3.39   \n",
              "2  The operation is not allowed because the name ...              9.03   \n",
              "3                                                450              3.67   \n",
              "4  The issue is that the system is limiting the n...              5.71   \n",
              "5     You can add up to 100 log headers to your log.              4.61   \n",
              "6                                                 23              7.10   \n",
              "7  Okay, let's address why you can't add another ...              9.81   \n",
              "\n",
              "  model name  \n",
              "0  gemma3:1b  \n",
              "1  gemma3:1b  \n",
              "2  gemma3:1b  \n",
              "3  gemma3:1b  \n",
              "4  gemma3:1b  \n",
              "5  gemma3:1b  \n",
              "6  gemma3:1b  \n",
              "7  gemma3:1b  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save to Excel with formatting\n",
        "excel_path = \"Excel_Data/Gemma/prompt_timings_results_01.xlsx\"\n",
        "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
        "    df.to_excel(writer, sheet_name='PromptResults', index=False)\n",
        "    \n",
        "    # Apply some basic formatting\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets['PromptResults']\n",
        "    format_wrap = workbook.add_format({'text_wrap': True, 'valign': 'top'})\n",
        "    worksheet.set_column('A:A', 50, format_wrap)  # Question\n",
        "    worksheet.set_column('B:B', 70, format_wrap)  # Response\n",
        "    worksheet.set_column('C:C', 18)               # Running Time\n",
        "    worksheet.set_column('D:D', 15)               # Model Name\n",
        "\n",
        "# Display the DataFrame in notebook\n",
        "display(df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM72gT33YROTvw/iOzBozoS",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1VdmZgAAqHSHdNcYYJEosB3DZAlbdLq19",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "211fc957897a4b648c7472ffcf0dfb5c": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_bf6949f69ab3400b84d1a9bd92ac5d79",
            "msg_id": "",
            "outputs": [
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "AI Bot Response:\n",
                  " You can define up to 200 tracks in one ODF file.\n"
                ]
              }
            ]
          }
        },
        "5f0df221121e47b4a6cd814d71340b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cba980fdfb1442a8984e268808b01ff0",
            "style": "IPY_MODEL_5f20b1e6f1e14e9db57a5953092095c9",
            "tooltip": "Click to query AI bot"
          }
        },
        "5f20b1e6f1e14e9db57a5953092095c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "72fd9f9d6f4941c4bb4793aaffcf5380": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Question:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c5f4a113e5694f719b3de2937b53e7d3",
            "placeholder": "Enter your question here...",
            "style": "IPY_MODEL_7c57b26ff215460fb377bd9be3e0b71d",
            "value": "How many tracks can you define in one ODF?"
          }
        },
        "7c57b26ff215460fb377bd9be3e0b71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf6949f69ab3400b84d1a9bd92ac5d79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f4a113e5694f719b3de2937b53e7d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "cba980fdfb1442a8984e268808b01ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
